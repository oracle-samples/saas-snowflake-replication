{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285521b-7bc9-465c-b2e7-4b3bc6a73b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Copyright (c) 2025, Oracle and/or its affiliates.\n",
    "#Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "pip install ocifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea201182-764a-45a3-9a9d-d16b6ad3e2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import oci\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import time\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession,SQLContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882f4e8-d730-4274-9eaf-7df4b3c63439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefixes = [\n",
    "    \"fscmtopmodelam_finextractam_apbiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_arbiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_cebiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_exmbiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_fabiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_funbiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_glbiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_iexbiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_xlabiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_xlebiccextractam\",\n",
    "    \"fscmtopmodelam_finextractam_zxbiccextractam\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7800aa-d671-453d-be83-ac37ea93b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import oci\n",
    "# OCI Configuration\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('~/.oci/config', CONFIG_PROFILE)\n",
    "\n",
    "def get_text_secret(secret_ocid):\n",
    "    vault_client = oci.secrets.SecretsClient(config)\n",
    "    try:\n",
    "        secret_content = vault_client.get_secret_bundle(secret_ocid).data.secret_bundle_content.content\n",
    "        decrypted_secret_content = base64.b64decode(secret_content).decode(\"utf-8\")\n",
    "    except Exception as ex:\n",
    "        print(\"ERROR: failed to retrieve the secret content\", ex, flush=True)\n",
    "        raise\n",
    "    return secret_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272f286-a3e1-4823-8499-42d42c62d9de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "passowrd=get_text_secret(\"<oci vault secret>\")\n",
    "\n",
    "sfOptions = {\n",
    "    \"sfUrl\": \"<sf account>.snowflakecomputing.com:443\",\n",
    "    \"sfAccount\": \"<>\",\n",
    "    \"sfUser\": \"<>\",\n",
    "    \"sfPassword\": password,\n",
    "    \"sfRole\": \"<>\",\n",
    "    \"sfWarehouse\": \"<>\", \n",
    "    \"sfDatabase\": \"<<\", \n",
    "    \"sfSchema\": \"<>\",\n",
    "    \"createTableOptions\": \"(overwrite=true)\"\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa653f-4b75-43b1-8d23-6f6653a99408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def in_dataflow():\n",
    "    \"\"\"\n",
    "    Determine if we are running in OCI Data Flow by checking the environment.\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"HOME\") == \"/home/dataflow\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc014c78-1767-4fe1-8a2f-8f9217cc8cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataflow_spark_session(\n",
    "    app_name=\"DataFlow\", file_location=None, profile_name=None, spark_config={}\n",
    "):\n",
    "    \"\"\"\n",
    "    Get a Spark session in a way that supports running locally or in Data Flow.\n",
    "    \"\"\"\n",
    "    if in_dataflow():\n",
    "        spark_builder = SparkSession.builder.appName(app_name)\n",
    "        \n",
    "    else:\n",
    "        # Import OCI.\n",
    "        try:\n",
    "            import oci\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"You need to install the OCI python library to test locally\"\n",
    "            )\n",
    "        # Use defaults for anything unset.\n",
    "        if file_location is None:\n",
    "            file_location = oci.config.DEFAULT_LOCATION\n",
    "        if profile_name is None:\n",
    "            profile_name = oci.config.DEFAULT_PROFILE\n",
    "        # Load the config file.\n",
    "        try:\n",
    "            oci_config = oci.config.from_file(\n",
    "                file_location=file_location, profile_name=profile_name\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"You need to set up your OCI config properly to run locally\")\n",
    "            raise e\n",
    "        conf = SparkConf()\n",
    "        conf.set(\"fs.oci.client.auth.tenantId\", oci_config[\"tenancy\"])\n",
    "        conf.set(\"fs.oci.client.auth.userId\", oci_config[\"user\"])\n",
    "        conf.set(\"fs.oci.client.auth.fingerprint\", oci_config[\"fingerprint\"])\n",
    "        conf.set(\"fs.oci.client.auth.pemfilepath\", oci_config[\"key_file\"])\n",
    "        conf.set(\n",
    "            \"fs.oci.client.hostname\",\n",
    "            \"https://objectstorage.{0}.oraclecloud.com\".format(oci_config[\"region\"]),\n",
    "        )\n",
    "        spark_builder = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"AppBicc\")\\\n",
    "        .config(\"spark.log.level\", \"ERROR\") \\\n",
    "        .config(\"spark.jars\", \"/home/datascience/snowflake-jdbc-3.15.1.jar,/home/datascience/spark-snowflake_2.12-2.15.0-spark_3.2.jar\") \\\n",
    "        .config(conf=conf) \n",
    "    # Add in extra configuration.\n",
    "    for key, val in spark_config.items():\n",
    "        spark_builder.config(key, val)\n",
    "\n",
    "    # Create the Spark session.\n",
    "    session = spark_builder.getOrCreate()\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26664ec-5efb-4672-8d12-2adec7b8e854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = get_dataflow_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee3cd8-18ec-4d82-ba65-32ee51b7b578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of bicc prefixes\n",
    "def checkLisOfObjects(path_file):\n",
    "    for substring in prefixes:\n",
    "        if substring in path_file:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea2699-2a33-4c9f-8623-1d3827cde041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_filename(filename):\n",
    "    # Split the filename based on underscores\n",
    "    parts = filename.split('_')\n",
    "    \n",
    "    # Ensure there are at least four underscores in the filename\n",
    "    if len(parts) < 5:\n",
    "        return \"Invalid filename\"\n",
    "    \n",
    "    # Join the parts after the fourth underscore\n",
    "    new_filename = '_'.join(parts[4:])\n",
    "    \n",
    "    # Remove everything after the first hyphen\n",
    "    new_filename = new_filename.split('-')[0]\n",
    "    \n",
    "    return new_filename+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6ef5a-6373-4ac3-8bc6-92f3334e5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delegation_token_path(spark):\n",
    "    \"\"\"\n",
    "    Get the delegation token path when we're running in Data Flow.\n",
    "    \"\"\"\n",
    "    if not in_dataflow():\n",
    "        return None\n",
    "    token_key = \"spark.hadoop.fs.oci.client.auth.delegationTokenPath\"\n",
    "    token_path = spark.sparkContext.getConf().get(token_key)\n",
    "    if not token_path:\n",
    "        raise Exception(f\"{token_key} is not set\")\n",
    "    return token_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddf2a0-2aa2-40df-8002-e539c0a3473b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_authenticated_client(token_path, client, file_location=None, profile_name=None):\n",
    "    \"\"\"\n",
    "    Get an an authenticated OCI client.\n",
    "\n",
    "    Example: get_authenticated_client(token_path, oci.object_storage.ObjectStorageClient)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import oci\n",
    "        if not in_dataflow():\n",
    "            # We are running locally, use our API Key.\n",
    "            if file_location is None:\n",
    "                file_location = oci.config.DEFAULT_LOCATION\n",
    "            if profile_name is None:\n",
    "                profile_name = oci.config.DEFAULT_PROFILE\n",
    "            config = oci.config.from_file(file_location=file_location, profile_name=profile_name)\n",
    "            authenticated_client = client(config)\n",
    "        else:\n",
    "            # We are running in Data Flow, use our Delegation Token.\n",
    "            with open(token_path) as fd:\n",
    "                delegation_token = fd.read()\n",
    "            signer = oci.auth.signers.InstancePrincipalsDelegationTokenSigner(\n",
    "                delegation_token=delegation_token\n",
    "            )\n",
    "            authenticated_client = client(config={}, signer=signer)\n",
    "    except Exception as e:\n",
    "            print(\"Error in get_authenticated_client function\")\n",
    "            raise e\n",
    "    return authenticated_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c933ac-d87a-404e-9af7-26dc404e0362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unzipToBucket(file_contents, destination_bucket_name, object_storage, namespace):\n",
    "    with zipfile.ZipFile(io.BytesIO(file_contents)) as zip_ref:\n",
    "    # List contents of the zip file\n",
    "        for file_info in zip_ref.infolist():\n",
    "            extracted_file_content = zip_ref.read(file_info.filename)\n",
    "            # Upload extracted file to destination bucket\n",
    "            object_storage.put_object(namespace,destination_bucket_name,process_filename(file_info.filename),io.BytesIO(extracted_file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52fc61-3466-4b3d-9988-6fd3f98cbdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "\n",
    "def unzipGzToBucket(file_contents, destination_bucket_name, object_storage, namespace, original_filename):\n",
    "    # Use gzip to decompress the .gz file\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(file_contents)) as gz_ref:\n",
    "        extracted_file_content = gz_ref.read()\n",
    "        # Derive the filename by stripping the .gz extension\n",
    "        filename = original_filename[:-3] if original_filename.endswith('.gz') else original_filename\n",
    "        # Upload extracted file to destination bucket\n",
    "        object_storage.put_object(namespace, destination_bucket_name, process_filename(filename), io.BytesIO(extracted_file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88f20a-9b8f-49e9-902e-329c9b0fe2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#process to read just the zip(or gzip) files with the prefixes and unzip them into a new bucket.\n",
    "try:\n",
    "    import oci\n",
    "    source_bucket = '<OCI Bucket>'\n",
    "    source_namespace='<OCI Namespace>'\n",
    "    next_starts_with = None\n",
    "    token_path = get_delegation_token_path(spark)\n",
    "    object_storage = get_authenticated_client(token_path, oci.object_storage.ObjectStorageClient)\n",
    "except Exception as e:\n",
    "    print(\"Error in authentication\")\n",
    "    raise e\n",
    "response = object_storage.list_objects(source_namespace, source_bucket,start=next_starts_with, fields='size,timeCreated,timeModified,storageTier', retry_strategy=oci.retry.DEFAULT_RETRY_STRATEGY)\n",
    "next_starts_with= response.data.next_start_with\n",
    "try:                         \n",
    "    start_time = time.time()\n",
    "    print(start_time)\n",
    "    for object_file in response.data.objects:\n",
    "            path_file=str(object_file.name)\n",
    "            if checkLisOfObjects(path_file):\n",
    "                get_object_response = object_storage.get_object(source_namespace, source_bucket, path_file)\n",
    "                unzipGzToBucket(get_object_response.data.content, \"BucketToDelete\", object_storage, \"idprle0k7dv3\",path_file)\n",
    "    if not next_starts_with:\n",
    "        end_time = time.time()\n",
    "        elapsed_time= end_time - start_time\n",
    "        print(\"Elapsed time: \", elapsed_time)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98276b40-4526-41c1-94a5-de259229b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFiles(path_file,spark):\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"oci://<target bucket>@<OCI Namespace>/\"+path_file)\n",
    "        df.write \\\n",
    "          .format(\"net.snowflake.spark.snowflake\") \\\n",
    "          .options(**sfOptions) \\\n",
    "          .option(\"dbtable\", path_file[0:-4]) \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .save()\n",
    "    except Exception as e:\n",
    "        print(\"Error Message!!!!!!!\", e)\n",
    "        df = spark.read.format(\"csv\").option(\"delimiter\",\"|||\").option(\"header\", \"true\").load(\"oci://<target bucket>@<OCI Namespace>/\"+path_file)\n",
    "        df.write \\\n",
    "          .format(\"net.snowflake.spark.snowflake\") \\\n",
    "          .options(**sfOptions) \\\n",
    "          .option(\"dbtable\", path_file[0:-4]) \\\n",
    "          .mode(\"overwrite\") \\\n",
    "          .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434628ed-15c9-45dd-9d18-8641fedf3a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_tasks(function, q, spark):\n",
    "    while not q.empty():\n",
    "        value = q.get()\n",
    "        function(value,spark)\n",
    "        q.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5b12e-e902-4483-a5cd-c2c793b325f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "q = Queue()\n",
    "\n",
    "worker_count = 4\n",
    "\n",
    "try:\n",
    "    import oci\n",
    "    source_bucket = '<>'\n",
    "    source_namespace='<OCI Namespace>'\n",
    "    next_starts_with = None\n",
    "    token_path = get_delegation_token_path(spark)\n",
    "    object_storage = get_authenticated_client(token_path, oci.object_storage.ObjectStorageClient)\n",
    "except Exception as e:\n",
    "    print(\"Error in authentication\")\n",
    "    raise e\n",
    "response = object_storage.list_objects(source_namespace, source_bucket,start=next_starts_with, fields='size,timeCreated,timeModified,storageTier', retry_strategy=oci.retry.DEFAULT_RETRY_STRATEGY)\n",
    "next_starts_with= response.data.next_start_with\n",
    "#In this case, I'm using a Bucket that only contains the csv files we want to load, so no filtering.\n",
    "start_time = time.time()\n",
    "file_paths = [file_name.name for file_name in response.data.objects]\n",
    "for file in file_paths:\n",
    "    q.put(file)\n",
    "\n",
    "for i in range(worker_count):\n",
    "    t=Thread(target=run_tasks, args=(loadFiles,q,spark))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "q.join()\n",
    "end_time = time.time()\n",
    "elapsed_time= end_time - start_time\n",
    "print(\"Elapsed time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ac59c-dc31-4442-8dca-8c570dd40f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete process\n",
    "buckets = [\"<source bucket with csv files>\", \"source bucket with zip files\"]\n",
    "source_namespace='<OCI Namespace>'\n",
    "next_starts_with = None\n",
    "token_path = get_delegation_token_path(spark)\n",
    "object_storage = get_authenticated_client(token_path, oci.object_storage.ObjectStorageClient)\n",
    "for bucket in buckets:\n",
    "    response = object_storage.list_objects(source_namespace, bucket,start=next_starts_with, fields='size,timeCreated,timeModified,storageTier', retry_strategy=oci.retry.DEFAULT_RETRY_STRATEGY)\n",
    "    next_starts_with= response.data.next_start_with\n",
    "    #In this case, I'm using a Bucket that only contains the csv files we want to load, so no filtering.\n",
    "    try:                         \n",
    "        for object_file in response.data.objects:\n",
    "                path_file=str(object_file.name)\n",
    "                if checkLisOfObjects(path_file):\n",
    "                    object_storage.delete_object(source_namespace, bucket, path_file)\n",
    "                elif bucket==\"<source bucket with csv files>\":\n",
    "                    object_storage.delete_object(source_namespace, bucket, path_file)\n",
    "        if not next_starts_with:\n",
    "            success='{\"status\":\"200\"}'\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v3]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
